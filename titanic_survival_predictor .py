# -*- coding: utf-8 -*-
"""Titanic-Survival-Predictor

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qn6yWvJVITuca3ixn6LNBfQzYBKV7xy1
"""

import pandas as pd
import numpy as np

df = pd.read_csv("/content/Titanic-Dataset.csv")

# CLEANING THE DATASET

# Dropping not-useful cols :

df = df.drop(columns=["PassengerId","Name","Ticket","Cabin"],axis=1)

# Adding Parch and Sibsp as Family :

df['Family'] = df['Parch'] + df['SibSp']

df = df.drop(columns=["Parch","SibSp"],axis=1)

df.isnull().sum() # Values Missing :- Age : 177 , Cabin : 687 , Embarked : 2

# Imputing Age and Embarked Values :

df['Age'] = df['Age'].fillna(df['Age'].mean())
df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])

df.head()

!pip install xgboost

# TRAINING THE MODEL

from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix,accuracy_score

# Apply one-hot encoding to categorical columns
df_encoded = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)


X = df_encoded.iloc[:,1:]
y = df_encoded.iloc[:,0]

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

model = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    eval_metric='logloss'
)

model.fit(X_train,y_train)

from sklearn.metrics import classification_report, confusion_matrix

y_pred = model.predict(X_test)

confusion = confusion_matrix(y_test, y_pred)
print(f"Confusion Matrix as follows :\n\n{confusion}")

print("")
classi = classification_report(y_test, y_pred)
print(f"Classification report :\n\n {classi}")

accuracy = accuracy_score(y_test,y_pred)

print(f"Accuracy Score is {accuracy}")

import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='Survived', data=df)
plt.title('Survival Distribution')
plt.xlabel('Survived (1=Yes, 0=No)')
plt.ylabel('Count')
plt.show()

# FEATURE IMP
import matplotlib.pyplot as plt
import seaborn as sns

feature_names = X.columns
importances = model.feature_importances_

plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=feature_names)
plt.title('Feature Importance from XGBoost')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()

# TRY IT !!

def predict_custom(Pclass, Sex, Age, Family, Fare, Embarked):
    df_input = pd.DataFrame([{
        'Pclass': Pclass,
        'Age': Age,
        'Family' : Family,
        'Fare': Fare,
        'Sex_male': 1 if Sex == 'male' else 0,
        'Embarked_Q': 1 if Embarked == 'Q' else 0,
        'Embarked_S': 1 if Embarked == 'S' else 0,
    }])

    # Ensure the columns are in the same order as the training data
    df_input = df_input[X_train.columns]

    prediction = model.predict(df_input)[0]
    return "Survived" if prediction == 1 else "Did NOT Survive"

# Example
predict_custom(3, 'male', 22, 1, 7.25, 'S')